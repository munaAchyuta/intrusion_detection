{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 install xlrd >= 1.0.0\n",
    "#! pip3 install openpyxl\n",
    "#! pip3 install pattern\n",
    "#!pip install JapaneseTokenizer # dependency needs to be installed mecab_wrapper \n",
    "#!pip install tinysegmenter\n",
    "#! git clone https://github.com/stopwords-iso/stopwords-ja.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load module\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import codecs\n",
    "import string\n",
    "from pattern.en import parsetree\n",
    "from pattern.en import parse\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kraft Super bowl data</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sno</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>「私は、私は再び8だように行動し、Velveetaマックとディナーにチーズを食べたいです。し...</td>\n",
       "      <td>I want to act like I'm 8 again and eat cheese ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this_Ray私は病気で寝ていると私は眠ることができません。 「赤穂PO SIトラヴィスク...</td>\n",
       "      <td>this_Ray I'm sick in bed and I can't sleep. \"A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT JeffSchultzAJC：グッデルは、彼はまた、クラを掃除された理由のAFCタイト...</td>\n",
       "      <td>RT JeffSchultzAJC: Goodell defends being at Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>「グッデルは、彼はまた、クラフトのガレージ、プールを掃除された理由のAFCタイトルゲームはま...</td>\n",
       "      <td>Goodell defends being at Craft House night bef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RTのc8suchy：「WorldStarComedy：このSMOKE TORNADO CR...</td>\n",
       "      <td>RT c8suchy: \"WorldStar Comedy: This SMOKE TORN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Kraft Super bowl data  \\\n",
       "Sno                                                      \n",
       "1    「私は、私は再び8だように行動し、Velveetaマックとディナーにチーズを食べたいです。し...   \n",
       "2    this_Ray私は病気で寝ていると私は眠ることができません。 「赤穂PO SIトラヴィスク...   \n",
       "3    RT JeffSchultzAJC：グッデルは、彼はまた、クラを掃除された理由のAFCタイト...   \n",
       "4    「グッデルは、彼はまた、クラフトのガレージ、プールを掃除された理由のAFCタイトルゲームはま...   \n",
       "5    RTのc8suchy：「WorldStarComedy：このSMOKE TORNADO CR...   \n",
       "\n",
       "                                               English  \n",
       "Sno                                                     \n",
       "1    I want to act like I'm 8 again and eat cheese ...  \n",
       "2    this_Ray I'm sick in bed and I can't sleep. \"A...  \n",
       "3    RT JeffSchultzAJC: Goodell defends being at Cr...  \n",
       "4    Goodell defends being at Craft House night bef...  \n",
       "5    RT c8suchy: \"WorldStar Comedy: This SMOKE TORN...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "data = pd.read_excel('Unstructured Data Japanese.xls', encoding='utf-8', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Kraft Super bowl data', 'English'], dtype='object')\n",
      "Index(['Kraft Super bowl data'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "data.drop(['English'],axis=1,inplace=True)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some japanese text translated to english samples.\n",
    "\n",
    "I want to act like I'm 8 again and eat cheese for Velveeta Mack and dinner. But it really isn't cardio worth I'll have to do tomorrow\n",
    "\n",
    "this_Ray I'm sick in bed and I can't sleep. \"Ako PO SI Travis Craft\" pops in my head and I thought of you. I want to do adobo now...\n",
    "\n",
    "RT JeffSchultzAJC: Goodell defends being at Craft House night before the AFC title game why he was also cleaning Kula has not yet dealt with...\n",
    "\n",
    "Goodell defends being at Craft House night before the AFC title game has yet to deal with why he was also cleaning the craft garage and pool.\n",
    "\n",
    "RT c8suchy: \"WorldStar Comedy: This SMOKE TORNADO CRAZY AFedU + 00A0U + 00BDedU + 00B2U + 00AFedU + 00A0U + 00BDedU + 00B8U + 0082edU + 00A0U + 00BDedU + 00B8U + 0082 http://t.co/90vHMsnnnheez\n",
    "\n",
    "Another 30 minutes before my alarm goes off, the window glass detergent is hitting my window EDU+00A0U+00BDedU+00B8U+00A1edU+00A0U+00BDedU+00B8U+00A1 #boreoff U+270B\n",
    "\n",
    "............All natural crispy craft peanut butter is like a crack at alledU+00A0U+00BDedU+00B8U+008D\n",
    "\n",
    "La...... Enjoy Goodell/Craft. The Fall of America-No boring name restrictions, entertainment or immorality\n",
    "\n",
    "I eat unhealthy pizza quantities and craft dinners\n",
    "\n",
    "Craft Mac and cheese have a ridiculous amount of protein in it because of what it is.\n",
    "\n",
    "RealSkipBayless Brady too cheats... and Goodell Craft... has done a zero dredibility fake analyst figuring out how to cover it...\n",
    "snooddood7 I want all the analysts who yelled at the swindler to provide their pinky with a white napkin folded into a craft.\n",
    "One place for drawing our fifty: Antidote to be boring! Check out our timeline short links & # 038. Input code 5yzAw 2 Billing aurora_kraft\n",
    "Weei Renesas cprice NFL craft seeking leaky cuz apology to media and criticism. Do not invest for itself. \"It's my job\" Doesn't apply here.\n",
    "RT ActuallyDontGAS: Kraft bkravitz sought you a FKN incompetent leak-free apology for investigation. You can't even get it RIGH...\n",
    "For the craft bkravitz survey you asked for an FKN sloppy leak-free apology. If you try, you can't even get it right.\n",
    "BostonGlobe fine is forced by craft demand for an apology that never flies... but a leak apology is appropriate\n",
    "It's important to make this distinction MikeReiss: Craft seeks apology for leaks, etc. NFL survey seeks apology for hes\n",
    "bkravitz craft wants all leak cuz apologies from the NFL. NFL should apologize for complicating with witch hunting makin\n",
    "RT shalisemyoung: Goodell ... asked about craft saying NE should get an apology if ball blame is false and said it was his job to protect INTE...\n",
    "Goodell said it's his job to protect the integrity of the game... and asked about craft saying NE should get an apology if the ball blame is false\n",
    "I honestly misunderstood Robert Kraft for craft cheese and I wondered why craft cheese would apologize for the alleged soccer claim.\n",
    "RT Jabba_Jabba_Jaw: Hot craft singles in your area open up the value of effort and are not really difficult.\n",
    "RT Jabba_Jabba_Jaw: Hot craft singles in your area open up the value of effort and are not really difficult.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sno</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>「私は、私は再び8だように行動し、Velveetaマックとディナーにチーズを食べたいです。し...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this_Ray私は病気で寝ていると私は眠ることができません。 「赤穂PO SIトラヴィスク...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT JeffSchultzAJC：グッデルは、彼はまた、クラを掃除された理由のAFCタイト...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>「グッデルは、彼はまた、クラフトのガレージ、プールを掃除された理由のAFCタイトルゲームはま...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RTのc8suchy：「WorldStarComedy：このSMOKE TORNADO CR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "Sno                                                   \n",
       "1    「私は、私は再び8だように行動し、Velveetaマックとディナーにチーズを食べたいです。し...\n",
       "2    this_Ray私は病気で寝ていると私は眠ることができません。 「赤穂PO SIトラヴィスク...\n",
       "3    RT JeffSchultzAJC：グッデルは、彼はまた、クラを掃除された理由のAFCタイト...\n",
       "4    「グッデルは、彼はまた、クラフトのガレージ、プールを掃除された理由のAFCタイトルゲームはま...\n",
       "5    RTのc8suchy：「WorldStarComedy：このSMOKE TORNADO CR..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rename(columns={'Kraft Super bowl data': 'text'}, inplace=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['これ', 'それ', 'あれ', 'この', 'その', 'あの', 'ここ', 'そこ', 'あそこ', 'こちら']\n",
      "['ほとんど', 'と共に', 'といった', 'です', 'とも', 'ところ', 'ここ', '「', '」', '：']\n"
     ]
    }
   ],
   "source": [
    "ja_stopwords = []\n",
    "\n",
    "with open('stopwords-ja/raw/japanese-stopwords.txt', encoding=\"utf8\",mode='r') as f:\n",
    "    stopwords_tmp = f.readlines()\n",
    "ja_stopwords.extend(stopwords_tmp)\n",
    "with open('stopwords-ja/raw/gh-stopwords-json-ja.txt', encoding=\"utf8\",mode='r') as f:\n",
    "    stopwords_tmp = f.readlines()\n",
    "ja_stopwords.extend(stopwords_tmp)\n",
    "with open('stopwords-ja/raw/ranksnl-japanese.txt', encoding=\"utf8\",mode='r') as f:\n",
    "    stopwords_tmp = f.readlines()\n",
    "ja_stopwords.extend(stopwords_tmp)\n",
    "with open('stopwords-ja/raw/bbalet_stopwords_ja.txt', encoding=\"utf8\",mode='r') as f:\n",
    "    stopwords_tmp = f.readlines()\n",
    "ja_stopwords.extend(stopwords_tmp)\n",
    "\n",
    "ja_stopwords = [i.strip('\\n').strip('\\t').strip(' ') for i in ja_stopwords]\n",
    "\n",
    "ja_stopwords.extend(['「','」','：'])\n",
    "\n",
    "print(ja_stopwords[:10])\n",
    "print(ja_stopwords[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string.punctuation\n",
    "# japanese puntuations\n",
    "ja_punctuations = \"\"\"’！ \"＃$％＆\\ '（）* +、-。/ :; <=>？@\" \\\\ \"^ _` {|}〜'!\"\"\"+string.punctuation\n",
    "en_alphabets = \"\"\"QWERTYUIOPASDFGHJKLZXCVBNMqwertyuiopasdfghjklzxcvbnm\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '0', '日', '放送', 'の', '「', '中居', '正広', 'の', 'ミ', 'に', 'なる', '図書館', '」', '（', 'テレビ', '朝日', '系', '）', 'で', '、', 'SMAP', 'の', '中居', '正広', 'が', '、', '篠原', '信一', 'の', '過去', 'の', '勘違い', 'を', '明かす', '一幕', 'が', 'あっ', 'た', '。']\n",
      "['日', '放送', '中居', '正広', 'ミ', '図書館', 'テレビ', '朝日', '系', 'SMAP', '中居', '正広', '篠原', '信一', '過去', '勘違い', '明かす', '一幕']\n"
     ]
    }
   ],
   "source": [
    "import tinysegmenter\n",
    "segmenter = tinysegmenter.TinySegmenter()\n",
    "tokens = segmenter.tokenize(u\"10日放送の「中居正広のミになる図書館」（テレビ朝日系）で、SMAPの中居正広が、篠原信一の過去の勘違いを明かす一幕があった。\")\n",
    "print(tokens)\n",
    "tokens_without_stopwords = [i for i in tokens if i not in ja_stopwords and i not in ja_punctuations and not i.isdigit()]\n",
    "print(tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above text and it's tokenizations translation\n",
    "\n",
    "#Masahiro Nakai of SMAP revealed a misunderstanding of Shin'ichi Shinohara's past misunderstandings on the \n",
    "#10th broadcast \"Masahiro Nakai's Mi no Mi Naru Library\" (TV Asahi).\n",
    "#--------- tokens\n",
    "#['1', '0','day','broadcast','no',''',' Nakai',' Masahiro','no',' Mi',' become',' become',' Library',''',' (',\n",
    "#'TV','Asahi','series',')','de',',','SMAP','no','Nakai','Masahiro' ,'Ga',',','shinohara','shinichi','no','past',\n",
    "#'no','misunderstanding','reveal','one act','ga', 'there were', '. ']\n",
    "#--------- toekns without stopwords\n",
    "#['1', '0',' Sun',' Broadcast',''',' Nakai',' Masahiro',' Mi',' Library',''',' (',' TV', ' Asahi','series',')',',',\n",
    "#'SMAP','Nakai','Masahiro',',','Shinohara','Shinichi','past','misunderstanding','reveal' \",\" one act \",\". ']\n",
    "#--------- tokens without stopwords, digits, punctuations\n",
    "#['Sun','Broadcast','Nakai','Masahiro','Mi','Library','Television','Asahi','Affiliation','SMAP','Nakai',\n",
    "#'Masahiro',' Shinohara','Shinichi','Past','Misunderstanding','Clear','One act']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ja_tokenize_and_cleanse(text):\n",
    "    #replace strings between brackets and parenthesis with blank\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # replace repeated characters with single\n",
    "    text = re.sub('\\.+', '.', text)\n",
    "    \n",
    "    # replace urls with blank\n",
    "    text = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', '', text)\n",
    "    \n",
    "    tokens = segmenter.tokenize(u\"{}\".format(text))\n",
    "    tokens = [i.strip() for i in tokens]\n",
    "    tokens_without_stopwords = [i for i in tokens if i not in ja_stopwords and i not in ja_punctuations and not i.isdigit() and i not in en_alphabets]\n",
    "    \n",
    "    return tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.すべて', '自然', 'カリカリクラフトピーナッツバター', 'alledU', 'BDedU', '亀裂', 'なもの']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\".........すべての自然なカリカリクラフトピーナッツバターはalledU + 00A0U + 00BDedU + 00B8U + 008Dで亀裂のようなものです\"\"\"\n",
    "print(ja_tokenize_and_cleanse(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyse overview of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference Data , that needs to be taken care.\n",
    "    「私は、私は再び8だように行動し、Velveetaマックとディナーにチーズを食べたいです。しかし、それは本当に私は明日行う必要があるでしょう心肺価値はありません ![image.png](attachment:image.png)\n",
    "    this_Ray私は病気で寝ていると私は眠ることができません。 「赤穂PO SIトラヴィスクラフトは、「私の頭の中にポップと私はあなたのことを考えました。私は今アドーボーをしたいです... ![image.png](attachment:image.png)\n",
    "    RT JeffSchultzAJC：グッデルは、彼はまた、クラを掃除された理由のAFCタイトルゲームはまだ対処していない前に、クラフトの家の夜にいる擁護します... ![image.png](attachment:image.png)\n",
    "    「グッデルは、彼はまた、クラフトのガレージ、プールを掃除された理由のAFCタイトルゲームはまだ対処していない前に、クラフトの家の夜にいる擁護しています。![image.png](attachment:image.png)\n",
    "    RTのc8suchy：「WorldStarComedy：このSMOKE TORNADO CRAZY AFedU + 00A0U + 00BDedU + 00B2U + 00AFedU + 00A0U + 00BDedU + 00B8U + 0082edU + 00A0U + 00BDedU + 00B8U + 0082 http://t.co/90vHMnnHUz\"tayy_shee18 ![image.png](attachment:image.png)\n",
    "    ............すべての自然なカリカリクラフトピーナッツバターはalledU + 00A0U + 00BDedU + 00B8U + 008Dで亀裂のようなものです ![image.png](attachment:image.png)\n",
    "    「私は不健康なピザの量とクラフトの夕食を食べます ![image.png](attachment:image.png)\n",
    "    「クラフトカナダ：新しい印刷用クーポンが利用可能！シェイクN '焼く澪とジェロhttp://t.co/7bErQm74Amに保存 ![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "#### Observations:\n",
    "    data has \n",
    "    1. http urls, that needs to be removed.\n",
    "    2. image attachment name or string needs to be removed\n",
    "    3. don't have much knowledge on this language. language understanding can help to explore more on this. for now it's blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleansed_tokens'] = data['text'].apply(lambda x: ja_tokenize_and_cleanse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「私は、私は再び8だように行動し、Velveetaマックとディナーにチーズを食べたいです。しかし、それは本当に私は明日行う必要があるでしょう心肺価値はありません  ---  ['再び', '行動', 'Velveeta', 'マック', 'ディナー', 'チーズ', '食べ', 'たい', '本当', '明日', '行う', '必要', 'でしょ', '心肺', '価値', 'ませ']  --- \n",
      "\n",
      "this_Ray私は病気で寝ていると私は眠ることができません。 「赤穂PO SIトラヴィスクラフトは、「私の頭の中にポップと私はあなたのことを考えました。私は今アドーボーをしたいです...  ---  ['this', 'Ray', '病気', '寝', '眠る', 'ませ', '赤穂', 'PO', 'SI', 'トラヴィスクラフト', '頭', '中', 'ポップ', 'あなた', '考え', 'まし', '今', 'アドーボー', 'たい']  --- \n",
      "\n",
      "RT JeffSchultzAJC：グッデルは、彼はまた、クラを掃除された理由のAFCタイトルゲームはまだ対処していない前に、クラフトの家の夜にいる擁護します...  ---  ['JeffSchultzAJC', 'グッデル', 'クラ', '掃除', '理由', 'AFC', 'タイトルゲーム', 'まだ', '対処', '前', 'クラフト', '家', '夜', '擁護']  --- \n",
      "\n",
      "「グッデルは、彼はまた、クラフトのガレージ、プールを掃除された理由のAFCタイトルゲームはまだ対処していない前に、クラフトの家の夜にいる擁護しています。  ---  ['グッデル', 'クラフト', 'ガレージ', 'プール', '掃除', '理由', 'AFC', 'タイトルゲーム', 'まだ', '対処', '前', 'クラフト', '家', '夜', '擁護']  --- \n",
      "\n",
      "RTのc8suchy：「WorldStarComedy：このSMOKE TORNADO CRAZY AFedU + 00A0U + 00BDedU + 00B2U + 00AFedU + 00A0U + 00BDedU + 00B8U + 0082edU + 00A0U + 00BDedU + 00B8U + 0082 http://t.co/90vHMnnHUz\"tayy_shee18  ---  ['suchy', 'WorldStarComedy', 'SMOKE', 'TORNADO', 'CRAZY', 'AFedU', 'BDedU', 'AFedU', 'BDedU', 'edU', 'BDedU', 'tayy', 'shee']  --- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rec in data[:5].iterrows():\n",
    "    print(rec[1]['text'],\" --- \",rec[1]['cleansed_tokens'],\" --- \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ウ', 144), ('Kラft', 127), ('クラフト', 98), ('ロベrtKラft', 67), ('ドン', 67), ('ビll', 67), ('スペrボwlXLイX', 67), ('メss', 66), ('Wイth', 66), ('RTムシc', 66), ('tヘ', 65), ('オ/GvHSp', 64), ('ア', 55), ('あなた', 54), ('チーズ', 52), ('ませ', 52), ('ト', 45), ('まし', 40), ('エd', 39), ('オン', 38), ('オ', 35), ('アンp', 34), ('BDedU', 31), ('fオr', 31), ('必要', 30), ('アンd', 30), ('グッデル', 25), ('レcイペs', 24), ('イン', 22), ('オオキングpゴオd', 22), ('Velveeta', 21), ('すべて', 21), ('velveeta', 21), ('エアr', 21), ('謝罪', 20), ('\\u200b', 20), ('イs', 20), ('ヨウ', 19), ('BD', 19), ('たい', 18), ('イt', 18), ('持っ', 17), ('NFL', 17), ('場合', 17), ('fロン', 17), ('ヂンネr', 17), ('オウt', 16), ('tハt', 16), ('bkravitz', 15), ('多く', 15)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "terms_all = [word for wordlist in data['cleansed_tokens'] for word in wordlist]\n",
    "count_all = Counter()\n",
    "# Update the counter\n",
    "count_all.update(terms_all)\n",
    "# Print the first 5 most frequent words --> meaningful phrases\n",
    "print(count_all.most_common(50))\n",
    "#print([i[0] for i in count_all.most_common(250)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English translated top words and phrases.\n",
    "\n",
    "['U','K Ra ft','Craft','Robe rt K Ra ft','Don','Bill','Super Bo wlXL Lee X','Me ss','W th' ,'RT MUSHI c','t HE','O/GvHSp','A','You','Cheese','Not','TO','Better','E d','ON'. ,'O','Ann p','BDedU','f o r','Need','Ann d','Goodell','Le c ipe s','In','Oh king p Goo d' ,'Velveeta','all','velveeta','air r','apology','\\u200b','i','you','BD','tai','it',' Have','nfl','case','fron','dine r','oh t','t ha t','bkravitz','more','wa tch','video',' 'T','l ike','heese','craft cheese','love rt','oh f','you r','...','edU','make','shit',' Kraft','They','Pat Rio ts','Goode ll','Ne d','Page','Ho w','Masuka','EDU','Bobcraft','Visit' ','De l c sulfur s','idea s','i','pe ch','id d','bane g','previous','angry','just',' t hi s','hi s','sw','more','oo ds','un f eye rly','attack d','dinner','via','like',' k la ft','ha s','yun y','ne w','wa lma rt','do','middle','false','work','a','nata ',' game','・','target','print','shell','do','n y','e ps','bamboo s','video s','mac' ,'w ll','eat','method','survey','get','say']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541 Number of tweets has 2671 words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1ン</th>\n",
       "      <th>abby</th>\n",
       "      <th>about</th>\n",
       "      <th>ac</th>\n",
       "      <th>actuallydontgas</th>\n",
       "      <th>aedu</th>\n",
       "      <th>afc</th>\n",
       "      <th>afcc</th>\n",
       "      <th>afedu</th>\n",
       "      <th>aint</th>\n",
       "      <th>...</th>\n",
       "      <th>飛ぶ</th>\n",
       "      <th>食べ</th>\n",
       "      <th>食べる</th>\n",
       "      <th>食べ物</th>\n",
       "      <th>食事</th>\n",
       "      <th>食品</th>\n",
       "      <th>養う</th>\n",
       "      <th>高価</th>\n",
       "      <th>魔女</th>\n",
       "      <th>麻疹</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2671 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1ン  abby  about  ac  actuallydontgas  aedu  afc  afcc  afedu  aint  ...  \\\n",
       "0   0     0      0   0                0     0    0     0      0     0  ...   \n",
       "1   0     0      0   0                0     0    0     0      0     0  ...   \n",
       "2   0     0      0   0                0     0    1     0      0     0  ...   \n",
       "3   0     0      0   0                0     0    1     0      0     0  ...   \n",
       "4   0     0      0   0                0     0    0     0      2     0  ...   \n",
       "\n",
       "   飛ぶ  食べ  食べる  食べ物  食事  食品  養う  高価  魔女  麻疹  \n",
       "0   0   1    0    0   0   0   0   0   0   0  \n",
       "1   0   0    0    0   0   0   0   0   0   0  \n",
       "2   0   0    0    0   0   0   0   0   0   0  \n",
       "3   0   0    0    0   0   0   0   0   0   0  \n",
       "4   0   0    0    0   0   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 2671 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most frequently occuring terms\n",
    "countVectorizer = CountVectorizer() \n",
    "countVector = countVectorizer.fit_transform([' '.join(text) for text in data['cleansed_tokens']])\n",
    "print('{} Number of tweets has {} words'.format(countVector.shape[0], countVector.shape[1]))\n",
    "#print(countVectorizer.get_feature_names())\n",
    "\n",
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n",
    "count_vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>531</th>\n",
       "      <th>532</th>\n",
       "      <th>533</th>\n",
       "      <th>534</th>\n",
       "      <th>535</th>\n",
       "      <th>536</th>\n",
       "      <th>537</th>\n",
       "      <th>538</th>\n",
       "      <th>539</th>\n",
       "      <th>540</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1ン</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abby</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuallydontgas</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 541 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0    1    2    3    4    5    6    7    8    9    ...  531  \\\n",
       "1ン                 0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "abby               0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "about              0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "ac                 0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "actuallydontgas    0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "\n",
       "                 532  533  534  535  536  537  538  539  540  \n",
       "1ン                 0    0    0    0    0    0    0    0    0  \n",
       "abby               0    0    0    0    0    0    0    0    0  \n",
       "about              0    0    0    0    0    0    0    0    0  \n",
       "ac                 0    0    0    0    0    0    0    0    0  \n",
       "actuallydontgas    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 541 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term-Document Matrix\n",
    "term_doc_df = count_vect_df.transpose()\n",
    "term_doc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc_df.index.name='Token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump as excel\n",
    "term_doc_df[:500].to_excel(\"tfidf-output-japanese.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl \n",
    "from openpyxl.styles import Font \n",
    "\n",
    "#wb = openpyxl.Workbook() \n",
    "wb = openpyxl.load_workbook(\"tfidf-output-japanese.xlsx\")\n",
    "sheet = wb.active \n",
    "\n",
    "sheet.insert_rows(0,1)\n",
    "\n",
    "# merge cell.\n",
    "sheet.merge_cells('B1:Z1') \n",
    "\n",
    "sheet.cell(row = 1, column = 2).value = 'S.No.(Unique Identifier of a comment.)'\n",
    "\n",
    "# set the font style to bold \n",
    "sheet.cell(row = 1, column = 2).font = Font(size = 24, bold = True) \n",
    "\n",
    "wb.save('tfidf-output-merge-japanese.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Exploratory Data Analysis\n",
    "\n",
    "    - these are general texts. still texts are quite unclean. some taken care here. still more required to be handled such as spell checker, more accurate phrase extracter using rule based parser or using NER. but langauge knowledge would have helped much.\n",
    "    - used Google translator to validate result. even some results have pasted to compare.\n",
    "    - due to lack of memory(3GB RAM) in my system, i'm dumping few samples of term-frequency records to excel. user can use same code to dump all records with having good memory system. or we can do other ways like using DASK, or writing file batch by batch.\n",
    "    - have mentioned comments with each code. i'm sure those are understandable. so not writing much here.\n",
    "    - with that 4 questions regarding to this task have been answered through code, comments, and image of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
