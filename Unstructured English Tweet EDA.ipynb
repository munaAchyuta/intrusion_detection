{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 install xlrd >= 1.0.0\n",
    "#! pip3 install openpyxl\n",
    "#! pip3 install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load module\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import codecs\n",
    "import string\n",
    "from pattern.en import parsetree\n",
    "from pattern.en import parse\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Katakana text Translated</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Huge @Patriots pep rally at Toso's in PHX...wa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For Robert Kraft and Roger Goodell air still i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For Robert Kraft and Roger Goodell air still i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Need #recipes? Visit the Kraft #CookingUpGood ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@EdgeofSports You take the good-You take the b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Katakana text Translated Unnamed: 2  Unnamed: 3\n",
       "ID                                                                          \n",
       "1   Huge @Patriots pep rally at Toso's in PHX...wa...        NaN         NaN\n",
       "2   For Robert Kraft and Roger Goodell air still i...        NaN         NaN\n",
       "3   For Robert Kraft and Roger Goodell air still i...        NaN         NaN\n",
       "4   Need #recipes? Visit the Kraft #CookingUpGood ...        NaN         NaN\n",
       "5   @EdgeofSports You take the good-You take the b...        NaN         NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "data = pd.read_excel('Unstructured Data English.xlsx', encoding='utf-8', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Katakana text Translated'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns\n",
    "data.drop(['Unnamed: 2','Unnamed: 3'],axis=1,inplace=True)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Huge @Patriots pep rally at Toso's in PHX...wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For Robert Kraft and Roger Goodell air still i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For Robert Kraft and Roger Goodell air still i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Need #recipes? Visit the Kraft #CookingUpGood ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@EdgeofSports You take the good-You take the b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "ID                                                   \n",
       "1   Huge @Patriots pep rally at Toso's in PHX...wa...\n",
       "2   For Robert Kraft and Roger Goodell air still i...\n",
       "3   For Robert Kraft and Roger Goodell air still i...\n",
       "4   Need #recipes? Visit the Kraft #CookingUpGood ...\n",
       "5   @EdgeofSports You take the good-You take the b..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rename(columns={'Katakana text Translated': 'text'}, inplace=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse overview of data"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference Data , that needs to be taken care.\n",
    "    Kraft Canada: New Printable Coupons Available! Save On Shake N ’ Bake MiO and Jell-O\n",
    "    Have you heard the latest about the Patriots? Bob Kraft paid the attendant to deflate the balls...in Singles. #thispunischeesy #onlykidding\n",
    "    @shalisemyoung @jmarsh4037 ask him if he'd rather hand the SB trophy to Marshawn/Sherman or Kraft/Brady #awkward\n",
    "![image.png](attachment:image.png)\n",
    "    RT @brijo20: The Pikes are seriously so sweet <ed><U+00A0><U+00BD><ed><U+00B8><U+008D>\n",
    "![image.png](attachment:image.png)\n",
    "    500 #0 6.5x10 KRAFT BUBBLE MAILERS PADDED MAILING ENVELOPE BAGS (Imperfect) http://t.co/mlR90zaEIR http://t.co/QqPXVOdQE9\n",
    "![image.png](attachment:image.png)\n",
    "    kraft's macaroni &amp; cheese &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; any other brand\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "#### Observations:\n",
    "    data has \n",
    "    1. http urls, that needs to be removed.\n",
    "    2. theme tags needs to be stripped to remove \"#\"\n",
    "    3. similarly user tags needs to be stripped to remove \"@\"\n",
    "    4. &amp; is used. which used to write “&” symbol in HTML. this needs to be replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['rt @marcobonzanini', 'an example', 'nlp // sam'], ['@marcobonzanini', 'example', '#nlp', 'sam'])\n"
     ]
    }
   ],
   "source": [
    "#cleanse data\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "#Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.extend(['rt','via',\"isn't\",\"ain't\",\"don't\",\"hasn't\",\"haven't\",\"I've\",\"I'VE\",\"i've\",'mr','mrs','Mr','Mrs','MR','MRS'])\n",
    "stop_words.extend([u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now'])\n",
    "stop_words = set(stop_words)\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def get_phrases(sentence):\n",
    "    phrase_dict = {}\n",
    "    parse_tree = parsetree(sentence, relations=True, lemmata=True)\n",
    "    for sent in parse_tree:\n",
    "        for chunk in sent.chunks:\n",
    "            phrase_key = chunk.type\n",
    "            phrase_val = ' '.join([w.string for w in chunk.words])\n",
    "\n",
    "            if not phrase_dict.get(phrase_key, []):\n",
    "                phrase_dict[phrase_key] = []\n",
    "            if len(phrase_val.split())>1:\n",
    "                phrase_dict[phrase_key].append(phrase_val)\n",
    "\n",
    "    return phrase_dict\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=True):\n",
    "    \n",
    "    # substitute hyper link with empty string\n",
    "    s = re.sub(r'http\\S+', '', s)\n",
    "    \n",
    "    # substitute html tabs with empty string\n",
    "    s = re.sub(r'<.*?>', '', s)\n",
    "    \n",
    "    #remove numbers from tweet\n",
    "    s = re.sub(r'\\w*\\d\\w*', '', s)\n",
    "    s = re.sub(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)','', s)\n",
    "    \n",
    "    #remove &amp; from tweet\n",
    "    s = s.replace('&amp;','&')\n",
    "    s = s.replace('’',\"'\")\n",
    "    s = s.replace('-',\" \")\n",
    "    \n",
    "    phrases = []\n",
    "    phrases = get_phrases(s).get('NP',[])\n",
    "    if lowercase:\n",
    "        phrases = [word_p.lower() for word_p in phrases]\n",
    "    \n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    \n",
    "    #check tokens against stop words , emoticons and punctuations\n",
    "    tokens = [word.replace(\"'s\",'') for word in tokens if word not in stop_words and word not in emoticons and word not in string.punctuation]\n",
    "    \n",
    "    #remove work with one or two chracter length.\n",
    "    tokens = [word for word in tokens if len(word)>2]\n",
    "    \n",
    "    tokens = [wn.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return (phrases, tokens)\n",
    " \n",
    "tweet = \"RT @marcobonzanini: just &amp; <abcd> an example! :D http://example.com #NLP 123 12/45/2020 987654321 SD09875 sam's\"\n",
    "print(preprocess(tweet))\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Huge @Patriots pep rally at Toso's in PHX...wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For Robert Kraft and Roger Goodell air still i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For Robert Kraft and Roger Goodell air still i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Need #recipes? Visit the Kraft #CookingUpGood ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@EdgeofSports You take the good-You take the b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "ID                                                   \n",
       "1   Huge @Patriots pep rally at Toso's in PHX...wa...\n",
       "2   For Robert Kraft and Roger Goodell air still i...\n",
       "3   For Robert Kraft and Roger Goodell air still i...\n",
       "4   Need #recipes? Visit the Kraft #CookingUpGood ...\n",
       "5   @EdgeofSports You take the good-You take the b..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['cleansed_text'] = data['text'].apply(lambda x: preprocess(x))\n",
    "data[['phrases', 'cleansed_tokens']] = data.apply(lambda row: pd.Series(preprocess(row['text'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huge @Patriots pep rally at Toso's in PHX...waiting for Mr Kraft's arrival #GoPats @PatriotsExtra http://t.co/y9NElMN6jo  ---  ['huge', '@patriots', 'pep', 'rally', 'toso', 'phx', 'waiting', 'kraft', 'arrival', '#gopats', '@patriotsextra']  ---  ['huge @patriots pep rally', 'mr kraft']\n",
      "\n",
      "For Robert Kraft and Roger Goodell air still isn't clear - Boston Globe http://t.co/Rw4oh3hswK #google #sport  ---  ['robert', 'kraft', 'roger', 'goodell', 'air', 'still', 'clear', 'boston', 'globe', '#google', '#sport']  ---  ['robert kraft and roger goodell air', \"n't clear boston globe\"]\n",
      "\n",
      "For Robert Kraft and Roger Goodell air still isn't clear - Boston Globe http://t.co/yN68nBRFne #google #sport  ---  ['robert', 'kraft', 'roger', 'goodell', 'air', 'still', 'clear', 'boston', 'globe', '#google', '#sport']  ---  ['robert kraft and roger goodell air', \"n't clear boston globe\"]\n",
      "\n",
      "Need #recipes? Visit the Kraft #CookingUpGood page &amp; watch the video for delicious ideas! http://t.co/eEkfmtgln4  ---  ['need', '#recipes', 'visit', 'kraft', '#cookingupgood', 'page', 'watch', 'video', 'delicious', 'idea']  ---  ['visit the kraft', 'cookingupgood page', 'the video', 'delicious ideas']\n",
      "\n",
      "@EdgeofSports You take the good-You take the bad-Givin Bob Kraft hand jobs ain't so bad-The Facts of Life-The Facts of Life#tootiebowl  ---  ['@edgeofsports', 'take', 'good', 'take', 'bad', 'givin', 'bob', 'kraft', 'hand', 'job', 'bad', 'fact', 'life', 'fact', 'life', '#tootiebowl']  ---  ['@edgeofsports you', 'the good you', 'the bad givin bob kraft hand jobs', 'the facts', 'life the facts']\n",
      "\n",
      "RT @HomespunSociety: Wrap it up http://t.co/82OdXf47m3 #startup #vancouver #toronto #handmadebot #trendit #share #homespunsoicety  ---  ['@homespunsociety', 'wrap', '#startup', '#vancouver', '#toronto', '#handmadebot', '#trendit', '#share', '#homespunsoicety']  ---  ['rt @homespunsociety', 'wrap it']\n",
      "\n",
      "@bseymour Cadbury got bought by Kraft a few years back. Hence this years Creame Eggs are terrible.  ---  ['@bseymour', 'cadbury', 'got', 'bought', 'kraft', 'year', 'back', 'hence', 'year', 'creame', 'egg', 'terrible']  ---  ['@bseymour cadbury', 'kraft a few years', 'this years creame eggs']\n",
      "\n",
      "http://t.co/4UFTZ1row3 enjoy  ---  ['enjoy']  ---  []\n",
      "\n",
      "RT @OTLonESPN: Tomorrow 8am ET ESPN2: Fallout from \"Deflategate\"reaction to Friday’s comments from Goodell  ---  ['@otlonespn', 'tomorrow', 'fallout', 'deflategate', 'reaction', 'friday', 'comment', 'goodell']  ---  ['tomorrow et']\n",
      "\n",
      "Tryna turn my kraft into cheddar  ---  ['tryna', 'turn', 'kraft', 'cheddar']  ---  ['my kraft']\n",
      "\n",
      "RT @kittyFitz50: I love Kraft beer .@America_On_Tap - Madison Ads on Limbaugh tarnish your image. #StopRush  ---  ['love', 'kraft', 'beer', '@america_on_tap', 'madison', 'ad', 'limbaugh', 'tarnish', 'image', '#stoprush']  ---  ['kraft beer', '@ america_on_tap madison ads', 'your image']\n",
      "\n",
      "Wrap it up http://t.co/82OdXf47m3 #startup #vancouver #toronto #handmadebot #trendit #share #homespunsoicety  ---  ['wrap', '#startup', '#vancouver', '#toronto', '#handmadebot', '#trendit', '#share', '#homespunsoicety']  ---  ['wrap it']\n",
      "\n",
      "Wrap it up http://t.co/Aqxa0vruhm #startup #vancouver #toronto #handmadebot #trendit #share #homespunsoicety  ---  ['wrap', '#startup', '#vancouver', '#toronto', '#handmadebot', '#trendit', '#share', '#homespunsoicety']  ---  ['wrap it']\n",
      "\n",
      "check this out For Robert Kraft and Roger Goodell air still isn't clear - Boston Globe: Bosto... http://t.co/fXF5wfilgt #sports #online  ---  ['check', 'robert', 'kraft', 'roger', 'goodell', 'air', 'still', 'clear', 'boston', 'globe', 'bosto', '#sports', '#online']  ---  ['robert kraft and roger goodell air', \"n't clear boston globe\"]\n",
      "\n",
      "The Wisdom I've Gleaned From Another Passing Year http://t.co/WXZ5mZpwZS  ---  ['wisdom', 'gleaned', 'another', 'passing', 'year']  ---  ['the wisdom i']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rec in data[:15].iterrows():\n",
    "    print(rec[1]['text'],\" --- \",rec[1]['cleansed_tokens'],\" --- \",rec[1]['phrases'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kraft', 5023), ('cheese', 1374), ('goodell', 906), ('velveeta', 802), ('robert', 744), ('video', 739), ('#cookingupgood', 645), ('#recipes', 633), ('new', 522), ('need', 468), ('patriot', 434), ('nfl', 426), ('watch', 417), ('like', 403), ('idea', 394), ('delicious', 391), ('roger', 389), ('visit', 387), ('page', 387), ('year', 386), ('mac', 359), ('food', 349), ('owner', 349), ('dinner', 321), ('apology', 320), ('pocket', 303), ('walmart', 290), ('yummy', 255), ('#sweepstakes', 253), ('bob', 250), ('deflategate', 239), ('make', 222), (\"i'm\", 207), ('integrity', 192), ('time', 190), ('get', 176), ('relationship', 169), ('say', 166), ('investigation', 166), ('speech', 159), ('bill', 156), ('marketing', 148), ('want', 146), ('data', 145), ('#nfl', 139), ('england', 136), ('said', 135), ('harnessed', 135), ('transform', 135), ('game', 130), ('know', 126), ('center', 123), ('stand', 123), ('front', 123), ('think', 122), ('one', 121), ('news', 121), ('big', 120), ('unfairly', 120), ('attacked', 120), ('#superbowlxlix', 118), ('spotlight', 116), ('love', 115), ('#kraft', 115), ('#robertkraft', 114), ('good', 113), ('got', 111), ('job', 110), ('mess', 110), ('bowl', 109), ('would', 109), ('recipe', 108), ('#rtmusic', 107), ('paper', 105), ('boston', 103), ('paid', 102), ('pat', 101), ('super', 100), ('belichick', 100), ('day', 99), ('#patriots', 98), ('million', 96), ('single', 95), ('team', 94), ('made', 90), ('right', 88), ('shell', 88), ('going', 87), ('man', 87), ('thing', 86), ('best', 86), ('great', 84), ('investment', 84), ('gutsy', 82), ('dip', 81), ('conference', 81), ('find', 79), ('@nflcommish', 79), ('really', 78), ('@patriots', 77), ('mailer', 77), ('help', 77), ('today', 76), ('eat', 76), ('medium', 76), ('probe', 76), ('bubble', 72), ('see', 72), ('box', 71), ('print', 71), ('#deflategate', 70), ('myers', 70), ('take', 69), ('real', 69), ('herald', 69), ('even', 66), ('look', 65), ('girlfriend', 64), ('card', 64), ('way', 64), ('chicken', 63), ('caleb', 63), ('modular', 63), ('thumbstick', 63), (\"can't\", 63), ('ever', 62), ('friend', 62), ('still', 61), ('gift', 61), ('extentions', 61), ('carroll', 61), ('back', 60), ('use', 60), ('come', 60), ('people', 59), ('last', 59), ('bag', 59), ('daily', 59), ('macaroni', 58), ('reuters', 58), ('never', 57), ('sherman', 57), ('brady', 57), ('gave', 56), ('stop', 56), ('coupon', 56), ('guy', 55), ('better', 55), ('york', 55), ('party', 55), ('protect', 55), ('asked', 55), ('life', 54), ('much', 54), ('padded', 54), ('commissioner', 54), ('work', 54), ('press', 54), ('american', 53), ('handicapped', 53), ('question', 53), ('@nfl', 53), ('envelope', 52), ('fan', 51), ('that', 51), ('meat', 51), ('absence', 51), ('lol', 50), ('shit', 50), ('sure', 49), ('gamers', 49), ('family', 49), ('house', 49), ('hand', 47), ('sunday', 47), ('caught', 47), ('put', 47), ('slice', 46), ('hate', 46), ('call', 45), ('week', 45), ('making', 44), ('well', 44), ('always', 44), ('old', 43), ('#superbowl', 43), ('someone', 43), ('cream', 43), ('league', 43), ('@bkravitz', 43), ('consumer', 43), ('check', 42), ('free', 42), ('engagement', 41), ('using', 41), ('enter', 41), ('ask', 41), ('could', 41), ('nothing', 41), ('entire', 40), ('jonathan', 39), ('pic', 39), ('lunchables', 39), ('win', 39), ('@youtube', 39), ('uff', 39), ('#gameday', 38), ('apologize', 38), ('girl', 38), ('buying', 38), ('eating', 38), ('let', 38), ('thought', 38), ('football', 38), ('found', 38), (\"i'll\", 38), ('#gossip', 38), ('@lovemyphilly', 37), ('story', 37), ('first', 37), ('demand', 37), ('pete', 37), ('home', 37), ('show', 37), ('saying', 37), ('wait', 37), ('sugar', 37), ('hot', 36), ('canadian', 36), ('twitter', 36), ('getting', 36), ('brown', 35), ('something', 35), ('wire', 35), ('photo', 35), ('as', 35), ('leak', 35), ('cup', 34), ('tell', 34), ('farm', 34), ('what', 34), ('sauce', 34), ('buy', 34), ('night', 34), ('ball', 33), ('next', 33), ('thanks', 33), ('antibiotic', 33), ('fuck', 33), ('kill', 33)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "terms_all = [word for wordlist in data['cleansed_tokens'] for word in wordlist]\n",
    "count_all = Counter()\n",
    "# Update the counter\n",
    "count_all.update(terms_all)\n",
    "# Print the first 5 most frequent words\n",
    "print(count_all.most_common(250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('robert kraft', 433), ('the video', 383), ('visit the kraft', 381), ('cookingupgood page', 381), ('delicious ideas', 381), ('kraft foods', 279), ('yummy new year', 249), ('roger goodell', 212), ('kraft dinner', 125), ('bob kraft', 109), ('my pocket velveeta', 109), ('my pockets velveeta', 106), ('an apology', 94), ('nfl spotlight', 88), ('the new england patriots', 85), ('million investment', 78), ('big time', 75), ('kraft cheese', 73), ('kraft apology', 72), ('patriots owner kraft', 70), ('deflategate investigation kraft relationship nfl integrity boston herald', 62), ('caleb kraft prints modular thumbstick extentions', 59), ('my job', 57), ('the nfl', 56), ('velveeta cheese', 52)]\n"
     ]
    }
   ],
   "source": [
    "terms_all_phrase = [word for wordlist in data['phrases'] for word in wordlist]\n",
    "count_all = Counter()\n",
    "# Update the counter\n",
    "count_all.update(terms_all_phrase)\n",
    "# Print the first 5 most frequent words --> meaningful phrases\n",
    "print(count_all.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count terms only once, equivalent to Document Frequency\n",
    "terms_single = set(terms_all)\n",
    "\n",
    "# Count hashtags only\n",
    "terms_hash = [term for term in terms_all if term.startswith('#')]\n",
    "\n",
    "# Count usertags only\n",
    "terms_usertag = [term for term in terms_all if term.startswith('@')]\n",
    "\n",
    "# Count terms only (no hashtags, no mentions)\n",
    "terms_only = [term for term in terms_all if not term.startswith(('#', '@'))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#cookingupgood', 645), ('#recipes', 633), ('#sweepstakes', 253), ('#nfl', 139), ('#superbowlxlix', 118), ('#kraft', 115), ('#robertkraft', 114), ('#rtmusic', 107), ('#patriots', 98), ('#deflategate', 70), ('#superbowl', 43), ('#gameday', 38), ('#gossip', 38), ('#news', 28), ('#goodell', 19), ('#coupon', 18), ('#cheese', 17), ('#rogergoodell', 16), ('#ebay', 16), ('#askroger', 15), ('#velveeta', 13), ('#sports', 12), ('#forsale', 12), ('#coupons', 11), ('#immersionjournalism', 11)]\n"
     ]
    }
   ],
   "source": [
    "count_all = Counter()\n",
    "# Update the counter\n",
    "count_all.update(terms_hash)\n",
    "# Print the first 5 most frequent words --> primary theme tags\n",
    "print(count_all.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kraft', 5023), ('cheese', 1374), ('goodell', 906), ('velveeta', 802), ('robert', 744), ('video', 739), ('new', 522), ('need', 468), ('patriot', 434), ('nfl', 426), ('watch', 417), ('like', 403), ('idea', 394), ('delicious', 391), ('roger', 389), ('visit', 387), ('page', 387), ('year', 386), ('mac', 359), ('food', 349), ('owner', 349), ('dinner', 321), ('apology', 320), ('pocket', 303), ('walmart', 290)]\n"
     ]
    }
   ],
   "source": [
    "count_all = Counter()\n",
    "# Update the counter\n",
    "count_all.update(terms_only)\n",
    "# Print the first 5 most frequent words\n",
    "print(count_all.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6936 Number of tweets has 8824 words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>___blaackgirl</th>\n",
       "      <th>__mochahontas__</th>\n",
       "      <th>_alextvn</th>\n",
       "      <th>_casssshew</th>\n",
       "      <th>_dyslexic</th>\n",
       "      <th>_elllaaaa</th>\n",
       "      <th>_inamad</th>\n",
       "      <th>_jpea</th>\n",
       "      <th>_karinad</th>\n",
       "      <th>_kaylakaylee</th>\n",
       "      <th>...</th>\n",
       "      <th>zimmsy</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zinga</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zootennis</th>\n",
       "      <th>zrau</th>\n",
       "      <th>zyla</th>\n",
       "      <th>ños</th>\n",
       "      <th>ôlée</th>\n",
       "      <th>öck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8824 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ___blaackgirl  __mochahontas__  _alextvn  _casssshew  _dyslexic  _elllaaaa  \\\n",
       "0              0                0         0           0          0          0   \n",
       "1              0                0         0           0          0          0   \n",
       "2              0                0         0           0          0          0   \n",
       "3              0                0         0           0          0          0   \n",
       "4              0                0         0           0          0          0   \n",
       "\n",
       "   _inamad  _jpea  _karinad  _kaylakaylee  ...  zimmsy  zinc  zinga  zoomed  \\\n",
       "0        0      0         0             0  ...       0     0      0       0   \n",
       "1        0      0         0             0  ...       0     0      0       0   \n",
       "2        0      0         0             0  ...       0     0      0       0   \n",
       "3        0      0         0             0  ...       0     0      0       0   \n",
       "4        0      0         0             0  ...       0     0      0       0   \n",
       "\n",
       "   zootennis  zrau  zyla  ños  ôlée  öck  \n",
       "0          0     0     0    0     0    0  \n",
       "1          0     0     0    0     0    0  \n",
       "2          0     0     0    0     0    0  \n",
       "3          0     0     0    0     0    0  \n",
       "4          0     0     0    0     0    0  \n",
       "\n",
       "[5 rows x 8824 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most frequently occuring terms\n",
    "countVectorizer = CountVectorizer() \n",
    "countVector = countVectorizer.fit_transform([' '.join(text) for text in data['cleansed_tokens']])\n",
    "print('{} Number of tweets has {} words'.format(countVector.shape[0], countVector.shape[1]))\n",
    "#print(countVectorizer.get_feature_names())\n",
    "\n",
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n",
    "count_vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6926</th>\n",
       "      <th>6927</th>\n",
       "      <th>6928</th>\n",
       "      <th>6929</th>\n",
       "      <th>6930</th>\n",
       "      <th>6931</th>\n",
       "      <th>6932</th>\n",
       "      <th>6933</th>\n",
       "      <th>6934</th>\n",
       "      <th>6935</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>___blaackgirl</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__mochahontas__</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_alextvn</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_casssshew</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_dyslexic</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6936 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0     1     2     3     4     5     6     7     8     9     \\\n",
       "___blaackgirl       0     0     0     0     0     0     0     0     0     0   \n",
       "__mochahontas__     0     0     0     0     0     0     0     0     0     0   \n",
       "_alextvn            0     0     0     0     0     0     0     0     0     0   \n",
       "_casssshew          0     0     0     0     0     0     0     0     0     0   \n",
       "_dyslexic           0     0     0     0     0     0     0     0     0     0   \n",
       "\n",
       "                 ...  6926  6927  6928  6929  6930  6931  6932  6933  6934  \\\n",
       "___blaackgirl    ...     0     0     0     0     0     0     0     0     0   \n",
       "__mochahontas__  ...     0     0     0     0     0     0     0     0     0   \n",
       "_alextvn         ...     0     0     0     0     0     0     0     0     0   \n",
       "_casssshew       ...     0     0     0     0     0     0     0     0     0   \n",
       "_dyslexic        ...     0     0     0     0     0     0     0     0     0   \n",
       "\n",
       "                 6935  \n",
       "___blaackgirl       0  \n",
       "__mochahontas__     0  \n",
       "_alextvn            0  \n",
       "_casssshew          0  \n",
       "_dyslexic           0  \n",
       "\n",
       "[5 rows x 6936 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term-Document Matrix\n",
    "term_doc_df = count_vect_df.transpose()\n",
    "term_doc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc_df.index.name='Token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump as excel\n",
    "term_doc_df[:50].to_excel(\"tfidf-output.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl \n",
    "from openpyxl.styles import Font \n",
    "\n",
    "#wb = openpyxl.Workbook() \n",
    "wb = openpyxl.load_workbook(\"tfidf-output.xlsx\")\n",
    "sheet = wb.active \n",
    "\n",
    "sheet.insert_rows(0,1)\n",
    "  \n",
    "# merge cell.\n",
    "sheet.merge_cells('B1:Z1') \n",
    "  \n",
    "sheet.cell(row = 1, column = 2).value = 'S.No.(Unique Identifier of a comment.)'\n",
    "\n",
    "# set the font style to bold \n",
    "sheet.cell(row = 1, column = 2).font = Font(size = 24, bold = True) \n",
    "  \n",
    "wb.save('tfidf-output-merge.xlsx') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Exploratory Data Analysis\n",
    "\n",
    "    - tweets are quite unclean. some taken care here. still more required to be handled such as spell checker, more accurate phrase extracter using rule based parser or using NER..etc.\n",
    "    - Most of the short cuts or abbreviations can either be transformed or dropped. Needs to be taken care after Subject knowledge.\n",
    "    - used lemmatizer, in this dataset.\n",
    "    - due to lack of memory(3GB RAM) in my system, i'm dumping few samples of term-frequency records to excel. user can use same code to dump all records with having good memory system. or we can do other ways like using DASK, or writing file batch by batch.\n",
    "    - have mentioned comments with each code. i'm sure those are understandable. so not writing much here.\n",
    "    - with that 4 questions regarding to this task have been answered through code, comments, and image of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NP': ['outlook application', 'my spam email'], 'VP': ['is not working'], 'PP': []}\n",
      "{'NP': ['Kraft beer', 'Madison Ads', 'your image'], 'VP': [], 'PP': []}\n",
      "{'NP': ['Robert Kraft and Roger Goodell air', 'Boston Globe'], 'ADVP': [], 'PP': [], 'VP': ['still is'], 'ADJP': [\"n't clear\"]}\n",
      "{'NP': ['Tomorrow 8am ET ESPN2', 's comments'], 'VP': [], 'PP': []}\n",
      "{'NP': ['@EdgeofSports You', 'the good-You', 'the bad-Givin Bob Kraft hand jobs', 'Life-The Facts'], 'VP': [], 'ADVP': [\"n't so\"], 'PP': []}\n",
      "{'PP': [], 'NP': ['Robert Kraft and Roger Goodell air', 'Boston Globe http://t.co/Rw4oh3hswK'], 'VP': ['still is'], 'ADJP': [\"n't clear\"]}\n"
     ]
    }
   ],
   "source": [
    "#import itertools\n",
    "#from nltk import ngrams\n",
    "from pattern.en import parsetree\n",
    "\n",
    "def get_phrases(sentence):\n",
    "    phrase_dict = {}\n",
    "    parse_tree = parsetree(sentence, relations=True, lemmata=True)\n",
    "    for sent in parse_tree:\n",
    "        for chunk in sent.chunks:\n",
    "            phrase_key = chunk.type\n",
    "            phrase_val = ' '.join([w.string for w in chunk.words])\n",
    "\n",
    "            if not phrase_dict.get(phrase_key, []):\n",
    "                phrase_dict[phrase_key] = []\n",
    "            if len(phrase_val.split())>1:\n",
    "                phrase_dict[phrase_key].append(phrase_val)\n",
    "\n",
    "    return phrase_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"\"\"outlook application is not working on my spam email\"\"\"\n",
    "    print(get_phrases(text))\n",
    "    text = \"\"\"RT @kittyFitz50: I love Kraft beer .@America_On_Tap - Madison Ads on Limbaugh tarnish your image. #StopRush\"\"\"\n",
    "    print(get_phrases(text))\n",
    "    text = \"\"\"check this out For Robert Kraft and Roger Goodell air still isn't clear - Boston Globe: Bosto... http://t.co/fXF5wfilgt #sports #online\"\"\"\n",
    "    print(get_phrases(text))\n",
    "    text = \"\"\"RT @OTLonESPN: Tomorrow 8am ET ESPN2: Fallout from \"Deflategate\"reaction to Friday’s comments from Goodell\"\"\"\n",
    "    print(get_phrases(text))\n",
    "    text = \"\"\"@EdgeofSports You take the good-You take the bad-Givin Bob Kraft hand jobs ain't so bad-The Facts of Life-The Facts of Life#tootiebowl\"\"\"\n",
    "    print(get_phrases(text))\n",
    "    text = \"\"\"For Robert Kraft and Roger Goodell air still isn't clear - Boston Globe http://t.co/Rw4oh3hswK #google #sport\"\"\"\n",
    "    print(get_phrases(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re \n",
    "import sys \n",
    "import string\n",
    "import codecs \n",
    "import subprocess\n",
    "from pattern.en import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query outlook application is not working on my spam email\n",
      "dict_keys(['NP', 'VP', 'PP'])\n",
      "get_keywords(query) ['outlook application', 'spam email']\n",
      "dict_keys(['NP', 'VP', 'PP'])\n",
      "get_keywords(query, ['NP']) ['outlook application', 'spam email']\n",
      "extract_hashtag(query) []\n",
      "extract_users(query) []\n",
      "\n",
      "query RT @kittyFitz50: I love Kraft beer .@America_On_Tap - Madison Ads on Limbaugh tarnish your image. #StopRush\n",
      "dict_keys(['NP', 'VP', 'PP'])\n",
      "get_keywords(query) ['@kittyfitz50', 'kraft beer', 'madison ads', 'limbaugh', 'image', 'stoprush']\n",
      "dict_keys(['NP', 'VP', 'PP'])\n",
      "get_keywords(query, ['NP']) ['@kittyfitz50', 'kraft beer', 'madison ads', 'limbaugh', 'image', 'stoprush']\n",
      "extract_hashtag(query) ['#stoprush']\n",
      "extract_users(query) ['@kittyfitz50', '@america_on_tap']\n",
      "\n",
      "query check this out For Robert Kraft and Roger Goodell air still isn't clear - Boston Globe: Bosto... http://t.co/fXF5wfilgt #sports #online\n",
      "dict_keys(['NP', 'ADVP', 'PP', 'VP', 'ADJP'])\n",
      "get_keywords(query) ['check', 'robert kraft roger goodell air', 'boston globe', 'bosto', 'sports', \"n't clear\", 'online']\n",
      "dict_keys(['NP', 'ADVP', 'PP', 'VP', 'ADJP'])\n",
      "get_keywords(query, ['NP']) ['check', 'robert kraft roger goodell air', 'boston globe', 'bosto', 'sports']\n",
      "extract_hashtag(query) ['#sports', '#online']\n",
      "extract_users(query) []\n",
      "\n",
      "query RT @OTLonESPN: Tomorrow 8am ET ESPN2: Fallout from \"Deflategate\"reaction to Friday’s comments from Goodell\n",
      "dict_keys(['NP', 'PP'])\n",
      "get_keywords(query) ['@otlonespn', 'tomorrow 8am et espn2', 'fallout', 'deflategate\"reaction', 'friday', 'comments', 'goodell']\n",
      "dict_keys(['NP', 'PP'])\n",
      "get_keywords(query, ['NP']) ['@otlonespn', 'tomorrow 8am et espn2', 'fallout', 'deflategate\"reaction', 'friday', 'comments', 'goodell']\n",
      "extract_hashtag(query) []\n",
      "extract_users(query) ['@otlonespn']\n",
      "\n",
      "query @EdgeofSports You take the good-You take the bad-Givin Bob Kraft hand jobs ain't so bad-The Facts of Life-The Facts of Life#tootiebowl\n",
      "dict_keys(['NP', 'VP', 'ADVP', 'PP'])\n",
      "get_keywords(query) ['@edgeofsports', 'good-you', 'bad-givin bob kraft hand jobs', 'facts', 'life-the facts', 'life#tootiebowl']\n",
      "dict_keys(['NP', 'VP', 'ADVP', 'PP'])\n",
      "get_keywords(query, ['NP']) ['@edgeofsports', 'good-you', 'bad-givin bob kraft hand jobs', 'facts', 'life-the facts', 'life#tootiebowl']\n",
      "extract_hashtag(query) ['#tootiebowl']\n",
      "extract_users(query) ['@edgeofsports']\n",
      "\n",
      "query For Robert Kraft and Roger Goodell air still isn't clear - Boston Globe http://t.co/Rw4oh3hswK #google #sport\n",
      "dict_keys(['PP', 'NP', 'VP', 'ADJP'])\n",
      "get_keywords(query) ['robert kraft roger goodell air', 'boston globe', 'google', 'sport', \"n't clear\"]\n",
      "dict_keys(['PP', 'NP', 'VP', 'ADJP'])\n",
      "get_keywords(query, ['NP']) ['robert kraft roger goodell air', 'boston globe', 'google', 'sport']\n",
      "extract_hashtag(query) ['#google', '#sport']\n",
      "extract_users(query) []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re \n",
    "import sys \n",
    "import string\n",
    "import codecs \n",
    "import subprocess\n",
    "\n",
    "#filepath = os.path.abspath(os.path.dirname(__file__))\n",
    "\n",
    "#reload(sys)  \n",
    "#sys.setdefaultencoding('utf8')\n",
    "\n",
    "from pattern.en import parse\n",
    "\n",
    "stopwords = [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now']\n",
    "\n",
    "\n",
    "\n",
    "### START Keyword extraction ### \n",
    "\n",
    "def extract_link(text):\n",
    "\tregex = r'https?://[^\\s<>\"]+|www\\.[^\\s<>)\"]+'\n",
    "\tmatch = re.findall(regex, text)\n",
    "\tlinks = [] \n",
    "\tfor x in match: \n",
    "\t\tif x[-1] in string.punctuation: links.append(x[:-1])\n",
    "\t\telse: links.append(x)\n",
    "\treturn links\n",
    "\t\n",
    "def cleanup(query): \n",
    "\ttry:\n",
    "\t\turls = extract_link(\" \" + query + \" \")\n",
    "\t\tfor url in urls: \n",
    "\t\t\tquery = re.sub(url, \"\", query)\n",
    "\t\tq = query.strip()\n",
    "\texcept:\n",
    "\t\tq = query\n",
    "\tq = re.sub(' RT ', '', ' ' + q + ' ').strip() \n",
    "\treturn q \n",
    "\n",
    "\n",
    "def convert_tag_format(query): \n",
    "\tword = query.split(' ')\n",
    "\tpostag = [(x.split('/')[0], x.split('/')[1]) for x in word]\n",
    "\treturn postag \n",
    "\t\n",
    "\n",
    "def get_pos_tags(text): \n",
    "\ttagged_sent = parse(text)\t\n",
    "\treturn convert_tag_format(tagged_sent), tagged_sent\n",
    "\t\n",
    "def normalise(word):\n",
    "\tword = word.lower()\n",
    "\treturn word\n",
    "\n",
    "\n",
    "## conditions for acceptable word: length, stopword\n",
    "def acceptable_word(word):\n",
    "    accepted = bool(2 <= len(word) <= 40\n",
    "        and word.lower() not in stopwords)\n",
    "    return accepted\n",
    "\n",
    "## extract entity from BIO encoding \n",
    "def extract_entity(filetext):\n",
    "\tlast_entity = '' \n",
    "\tlast_tag = '' \n",
    "\tmention2entities = {} \n",
    "\tfor line in filetext.split('\\n'): \n",
    "\t\tline = line.strip() \n",
    "\t\tif line == '': \n",
    "\t\t\tcontinue\n",
    "\t\tline_split = line.split('\\t')\n",
    "\t\tif re.search('B-', line_split[1]): \n",
    "\t\t\tif last_entity != '': \n",
    "\t\t\t\tif not last_tag in mention2entities:\n",
    "\t\t\t\t\tmention2entities[last_tag] = [] \n",
    "\t\t\t\tmention2entities[last_tag].append(last_entity.strip())\n",
    "\t\t\tlast_entity = line_split[0] + ' '\n",
    "\t\t\tlast_tag = line_split[1][2:] \n",
    "\t\telif re.search('I-', line_split[1]): \n",
    "\t\t\tlast_entity += line_split[0] + ' '\n",
    "\tif last_entity != '': \n",
    "\t\tif not last_tag in mention2entities:\n",
    "\t\t\tmention2entities[last_tag] = [] \n",
    "\t\tmention2entities[last_tag].append(last_entity.strip())\n",
    "\treturn \tmention2entities\n",
    "\n",
    "\t\n",
    "def get_entities_from_phrase(tagged_sent, phrase2consider): \n",
    "\tword = tagged_sent.split(' ')\n",
    "\tbio_tags = [normalise(x.split('/')[0])+ '\\t'+ x.split('/')[2] for x in word]\n",
    "\tbio_text = '\\n'.join(bio_tags)\n",
    "\tmention2entities = extract_entity(bio_text)\n",
    "\tprint(mention2entities.keys())\n",
    "\t\n",
    "\t## strip off unacceptable words \n",
    "\t_mention2entities = {} \n",
    "\tfor mention in mention2entities: \n",
    "\t\tif not mention in phrase2consider: \n",
    "\t\t\tcontinue\n",
    "\t\t_mention2entities[mention] = [] \n",
    "\t\tfor entity in mention2entities[mention]: \n",
    "\t\t\t_entity = ' '.join([word for word in entity.split(' ') if acceptable_word(word)]).strip()\n",
    "\t\t\tif _entity != '': \n",
    "\t\t\t\t_mention2entities[mention].append(_entity)\n",
    "\t\t\t\n",
    "\tentities = []\n",
    "\tfor mention in _mention2entities: \n",
    "\t\tentities.extend(_mention2entities[mention])\n",
    "\treturn entities\t\n",
    "\t\n",
    "\n",
    "def get_keywords(text, phrase2consider=['NP', 'ADJP']): \n",
    "\t_text = cleanup(text)\n",
    "\ttry:\n",
    "\t\tpostoks, tagged_sent = get_pos_tags(_text)\n",
    "\t\tentities = get_entities_from_phrase(tagged_sent, phrase2consider)\n",
    "\texcept: \n",
    "\t\treturn []\n",
    "\treturn entities\n",
    "\n",
    "### END Keyword extraction ### \n",
    "\n",
    "### START other entity extraction ### \n",
    "def extract_hashtag(text, to_normalize=True):\n",
    "\tregex = r'#[^\\W\\d_]+\\b'\n",
    "\tif to_normalize: text = normalise(text)\n",
    "\tmatch = re.findall(regex, text)\n",
    "\treturn match\n",
    "\n",
    "def extract_users(text, to_normalize=True):\n",
    "#\tregex = r'@[^\\W\\d_]+\\b'\n",
    "\tregex = r'@[^\\b ]+\\b'\n",
    "\tif to_normalize: text = normalise(text)\n",
    "\tmatch = re.findall(regex, text)\n",
    "\treturn match\n",
    "\n",
    "\n",
    "def get_emoji_list():\n",
    "\temoji_text = codecs.open(filepath+'/emoji_table.txt', 'r', 'utf-8').read().split('\\n')\n",
    "\temojis = [x.split(',')[0] for x in emoji_text][1:]\n",
    "\temojis = [x for x in emojis if x.strip() != '']\n",
    "\treturn emojis\n",
    "#emojis = get_emoji_list()\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "#Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "#combine sad and happy emoticons\n",
    "emojis = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "def extract_emojis(text): \n",
    "\tglobal emojis\n",
    "\t_emos = [] \n",
    "\tfor each in emojis: \n",
    "\t\teach = each.strip() \n",
    "\t\tif each == '': continue\n",
    "\t\tif each in text: \n",
    "\t\t\t_emos.extend(re.findall(each, text.decode('utf-8')))\n",
    "\t\n",
    "#\t_emos2 = []\n",
    "#\tfor char in text.decode('utf-8'): \n",
    "#\t\tif char in _emos: \n",
    "#\t\t\t_emos2.append(char)\t\t\n",
    "\treturn _emos#, _emos2 \n",
    "\t\n",
    "### END other entity extraction ### \n",
    "\n",
    "def all_entities(text, to_normalize=True, with_unigram=True): \n",
    "\ttext = text.decode('utf-8')\n",
    "\tif to_normalize: text = normalise(text)\n",
    "\tif with_unigram: \n",
    "\t\tentities = text.split()\n",
    "\telse: \n",
    "\t\tentities = [] \n",
    "\tkeywords = get_keywords(text, ['NP', 'VP', 'ADJP', 'ADVP'])\n",
    "\temojis = extract_emojis(text)\n",
    "\tfor each in keywords+emojis: \n",
    "\t\tif not each in entities:\n",
    "\t\t\tentities.append(each)\n",
    "\treturn entities\n",
    "\t\n",
    "if __name__ == '__main__':\t\n",
    "\tqueries = [\"The mobile web is more important than mobile apps.\", \"As a #roadmapscholar, I highly recommend #startup bootcamp for #founders by @andrewsroadmaps : http://t.co/ZBISIMEBRH (http://t.co/VF5CojRWNF)\", \"RT @andrewsroadmaps: Proud of @naushadzaman &amp; @WasimKhal for winning the #IBMWatson hackathon! #roadmapscholars https://t.co/08sbAjKWKu.\"]\n",
    "\tqueries = [\"\"\"outlook application is not working on my spam email\"\"\",\"\"\"RT @kittyFitz50: I love Kraft beer .@America_On_Tap - Madison Ads on Limbaugh tarnish your image. #StopRush\"\"\",\"\"\"check this out For Robert Kraft and Roger Goodell air still isn't clear - Boston Globe: Bosto... http://t.co/fXF5wfilgt #sports #online\"\"\",\"\"\"RT @OTLonESPN: Tomorrow 8am ET ESPN2: Fallout from \"Deflategate\"reaction to Friday’s comments from Goodell\"\"\",\"\"\"@EdgeofSports You take the good-You take the bad-Givin Bob Kraft hand jobs ain't so bad-The Facts of Life-The Facts of Life#tootiebowl\"\"\",\"\"\"For Robert Kraft and Roger Goodell air still isn't clear - Boston Globe http://t.co/Rw4oh3hswK #google #sport\"\"\"]\n",
    "    \n",
    "\tfor query in queries: \n",
    "\t\tprint('query', query)\n",
    "\t\tprint('get_keywords(query)', get_keywords(query))\n",
    "\t\tprint(\"get_keywords(query, ['NP'])\", get_keywords(query, ['NP']))\n",
    "\t\tprint(\"extract_hashtag(query)\", extract_hashtag(query)) \n",
    "\t\tprint(\"extract_users(query)\", extract_users(query))\n",
    "\t\t#print \"extract_link(query)\", extract_link(query)\n",
    "\t\tprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.4-py2.py3-none-any.whl (241 kB)\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.0.1.tar.gz (8.4 kB)\n",
      "Collecting jdcal\n",
      "  Downloading jdcal-1.4.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Building wheels for collected packages: et-xmlfile\n",
      "  Building wheel for et-xmlfile (setup.py): started\n",
      "  Building wheel for et-xmlfile (setup.py): finished with status 'done'\n",
      "  Created wheel for et-xmlfile: filename=et_xmlfile-1.0.1-py3-none-any.whl size=8920 sha256=715dd79b729c4871bee51760b0ed100ed9feedaafb6e71ab5f28845b2c736f9b\n",
      "  Stored in directory: c:\\users\\achyuta\\appdata\\local\\pip\\cache\\wheels\\e2\\bd\\55\\048b4fd505716c4c298f42ee02dffd9496bb6d212b266c7f31\n",
      "Successfully built et-xmlfile\n",
      "Installing collected packages: et-xmlfile, jdcal, openpyxl\n",
      "Successfully installed et-xmlfile-1.0.1 jdcal-1.4.1 openpyxl-3.0.4\n"
     ]
    }
   ],
   "source": [
    "! pip3 install openpyxl\n",
    "#! pip3 install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
